<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> MiGA-IJCAI2024</title>

    <link rel="shortcut icon" href="assets/images/miga-mini.png">
    <link rel="stylesheet" href="assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="assets/css/fontawsom-all.min.css">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />
</head>

<style>

</style>

<body>
    <div class="container-fluid overcover">
        <div class="container profile-box">
            <div class="top-cover">
                <div class="covwe-inn">
                    <div class="row no-margin">
                        <div class="col-md-4 img-c">
                            <img src="assets/images/miga.png" alt="">
                        </div>
                        <div class="col-md-8 tit-det">
                            <h2>The 2nd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</h2>
                            <p> To be held at IJCAI 2024, 3th-9th August 2024, Jeju, South Korea

                        </div>
                    </div>
                </div>
            </div>


            <ul class="nav nav-tabs navbar-expand-lg navbar-dark nav-fill" id="myTab" role="tablist">
              <li class="nav-item">
                <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" id="profile-tab" data-toggle="tab" href="#profile" role="tab" aria-controls="profile" aria-selected="false">Workshop</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" id="profile-tab" data-toggle="tab" href="#resume" role="tab" aria-controls="profile" aria-selected="false">Challenge</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" id="profile-tab" data-toggle="tab" href="#gallery" role="tab" aria-controls="profile" aria-selected="false">Organisers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" id="contact-tab" data-toggle="tab" href="#contact" role="tab" aria-controls="contact" aria-selected="false">Contact</a>
              </li>
            </ul>
            <div class="tab-content" id="myTabContent">
              <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">
                  <div class="row no-margin home-det">
                      <div class="col-md-4 big-img">

                         <h4 class="ltitle">Welcome to MiGA Workshop & Challenge 2024</h4>
                        <!-- <ul class="hoby row no-margin">
                            <li><i class="fas fa-pencil-alt"></i> <br> Writing</li>
                            <li><i class="fas fa-bicycle"></i> <br> Cycling</li>
                            <li><i class="fas fa-futbol"></i> <br> Football</li>
                            <li><i class="fas fa-film"></i><br> Movies</li>
                            <li><i class="fas fa-plane-departure"></i> <br>Travel</li>
                            <li><i class="fas fa-gamepad"></i> <br> Games</li>
                        </ul> -->
                          We jointly hold the second workshop and challenges for Micro-gesture Analysis for Hidden Emotion Understanding (MiGA) at IJCAI 2024, 3rd-9th August 2024. We warmly welcome your contribution and participation.  <h4 class="ltitle">News</h4>
<!--
                        <div class="refer-cov">

                            <p><b>7 March</b> : Great news! The MiGA workshop has been accepted at IJCAI for 2023!</p>
                        </div>
                        <div class="refer-cov">

                            <p> <b>20 March</b> : The website of MiGA workshop & challenge is avaible.</p>
                        </div>

                        <div class="refer-cov">

                            <p> <b>27 March</b> : The Codalab website of MiGA challenge is avaible.</p>
                        </div>

                         <div class="refer-cov">

                            <p> <b>17 May</b> : The final results of MiGA challenge is avaible, congratulations to winners!</p>
                        </div>
                        <div class="refer-cov">

                            <p> <b>17 May</b> : The submission deadline of MiGA workshop is extended!</p>
                        </div> -->


                      </div>
                      <div class="col-md-8 home-dat" style="text-align: justify;">
                          <h2 class="rit-titl" > Overview</h2>


                          <p>  We hold the 2nd MiGA Workshop & Challenge to <b>explore using body gestures for hidden emotional state analysis</b>, to be held at IJCAI 2024.
                          </p>

                           <p>
                          As an important non-verbal communicative fashion, human body gestures are capable of conveying emotional information during social communication. In previous works, efforts have been made mainly on facial expressions, speech, or expressive body gestures to interpret classical expressive emotions. Differently, we focus on a specific group of body gestures, called micro-gestures (MGs), used in the psychology research field to interpret inner human feelings.

                          </p>
                                               <div class="profess-cover row no-margin">
                            <div class="col-md-6">

                            </div>
                        </div>

                         <div style="text-align: center;"> <img src="assets/dataset/iMiGUE_short2.gif" alt="Micro-gesture" width="500" height="300" class="center"></div>

                           <p style="padding-top: 10px;">    <b>
                          MGs are subtle and spontaneous body movements that are proven, together with micro-expressions, to be more reliable than normal facial expressions for conveying hidden emotional information.  </b>

                          </p>


                          <p>The aim of our MiGA workshop & challenge is to build a united, supportive research community for micro-gesture analysis and related emotion understanding problems. It will facilitate discussions between different research labs in academia and industry, identify the main attributes that can vary between gesture-based emotional understanding, and discuss the progress that has been made in this field so far, while identifying the next immediate open problems the community should address. We provide two different datasets and related benchmarks and with to inspire a new way of utilizing body gestures for human emotion understanding and bring a new direction to the emotion AI community. </p>



                        <div class="profess-cover row no-margin">
                            <div class="col-md-6">

                            </div>
                        </div>

                        <div>
                            <h4> Workshop Topics </h4>
                            <ul>
                              The workshop supplementing the challenge covers a wider scope, i.e., any paper that is related to gesture and micro-gesture analysis for emotion understanding but is not directly about any of the challenge tracks can be submitted as a workshop paper. The topic includes but is not limited to:
                              <div style="margin-left: 20px; text-align: justify;">
                              <li style="list-style: disc;">Gesture and micro-gesture analysis for emotion understanding.</li>
                                <li style="list-style: disc;">Vision-based methodologies for gesture-based emotion understanding, e.g., classification, detection, online recognition, generation, and transferring.</li>
                                <li style="list-style: disc;">Solutions for special challenges involved with the in-the-wild gesture analysis, e.g., severely imbalanced sample distribution, high heterogeneous samples of interclass, noisy irrelevant motions, noisy backgrounds, etc.</li>
                                <li style="list-style: disc;">Different modalities developed for emotion understanding, e.g., body gestures, attentive gazes, and desensitized voices.</li>
                                <li style="list-style: disc;">New data collected for the purpose of hidden emotion understanding.</li>
                                <li style="list-style: disc;">Psychological study and neuroscience research about various body behaviors and their links to emotions.</li>
                                <li style="list-style: disc;">Hardware/apparatuses/imaging systems developed for the purpose of hidden emotion understanding.</li>
                                <li style="list-style: disc;">Applications of gestures and micro-gestures, e.g., for medical assessment in hospitals (ADHD, depression), for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc.</li>
                              </div>
                              </ul>

                            <h4> References </h4>
                                  <ul>
                                    <li style="list-style: disc;">Chen H., Shi H., Liu X., Li X., and Zhao G. SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis. International Journal of Computer Vision (IJCV 2023), 2023: 1-21.</li>
                                    <li style="list-style: disc;">Liu, X., Shi H., Chen H., Yu Z., Li X., and Zhao G. "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021), pp. 10631-10642. 2021.</li>
                                    <li style="list-style: disc;">Chen H., Liu X., Li X., Shi H., & Zhao G. Analyze spontaneous gestures for emotional stress state recognition: A micro-gesture dataset and analysis with deep learning. 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019): 1-8.</li>

                              </ul>

                        </div>
                        <!-- <div class="links">
                          <div class="row ">
                              <div class="col-xl-6 col-md-12">
                                  <ul class="btn-link">
                                      <li>
                                          <a href=""><i class="fas fa-paper-plane"></i> Hire Me</a>
                                      </li>
                                      <li>
                                          <a href=""><i class="fas fa-cloud-download-alt"></i> Download Resume</a>
                                      </li>
                                  </ul>
                              </div>
                              <div class="col-xl-6 col-md-12">
                                  <ul class="social-link">
                                      <li><i class="fab fa-facebook-f"></i></li>
                                      <li><i class="fab fa-twitter"></i></li>
                                      <li><i class="fab fa-pinterest-p"></i></li>
                                      <li><i class="fab fa-linkedin-in"></i></li>
                                      <li><i class="fab fa-linkedin-in"></i></li>
                                      <li><i class="fab fa-youtube"></i></li>
                                  </ul>
                              </div>
                          </div>
                      </div> -->
                      <!-- <div class="jumbo-address">
                         <div class="row no-margin">
                                  <div class="col-lg-6 no-padding">

                                  <table class="addrss-list">
                                      <tbody><tr>
                                          <th>Position</th>
                                          <td>Freelance</td>
                                      </tr>
                                      <tr>
                                          <th>Nationality</th>
                                          <td>American</td>
                                      </tr>
                                      <tr>
                                          <th>Date of birth</th>
                                          <td>09-06-1989</td>
                                      </tr>
                                  </tbody></table>

                          </div>
                          <div class="col-lg-6 no-padding">
                               <table class="addrss-list">
                                      <tbody><tr>
                                          <th>Experiance</th>
                                          <td>5+ Years</td>
                                      </tr>
                                      <tr>
                                          <th>Website</th>
                                          <td>www.yourdomain.com</td>
                                      </tr>
                                      <tr>
                                          <th>Languages</th>
                                          <td>English,French,Germany</td>
                                      </tr>
                                  </tbody></table>
                          </div>
                         </div>

                      </div> -->

                      </div>
                  </div>
              </div>
              <div class="tab-pane fade exp-cover" id="profile" role="tabpanel" aria-labelledby="profile-tab">
                  <div class="data-box" style="text-align: justify;">
                        <div class="sec-title">
                        <h2 class="decorated"><span>Workshop Details</span></span></h2>
                        </div>

                          <h5> Introduction </h5>
                           <p> Body gestures are an important form to reveal people’s emotions alongside facial expressions and speeches. For special occasions when people intend to control or hide their true feelings, e.g., for social etiquette or other reasons, body gestures are harder to control and thus the more revealing clues of actual feelings compared to the face and the voice. Microgestures (MG) are defined as a special category of body gestures that are indicative of humans’ emotional status. Representative instances include: scratching head, touching nose, and rubbing hands, which are not intended to be shown when communicating with others, but occur spontaneously due to e.g., felt stress or discomfort as shown in Figure 1. The MGs differ from indicative gestures which are performed on purpose for facilitating communications, e.g., using gestures to assist verbal expressions during a discussion. Although research on general body gestures is prevailing, it is largely about human’s macro body movement, lacking the finer level consideration such as the micro-gestures discussed above. As well the studies are mainly concerned with recognizing the movement performed expressively, and the link between gestures and hidden emotions is yet to be explored.
                          </p>

                           <div class="profess-cover row no-margin">
                          <div class="col-md-6">

                          </div>
                          </div>

                          <div style="text-align: center;"> <img src="assets/dataset/iMiGUE_short2.gif" alt="Micro-gesture" width="500" height="300" class="center"></div>
                          <p>
                          Fig. 1. Micro-gesture examples. A tennis player is talking while spontanously performing the body gestures in the post-match interview.
                          </p>

                          <div class="profess-cover row no-margin">
                         <div class="col-md-6">

                         </div>
                         </div>
        <p>
        Motivated by the above observations, we propose to jointly hold the second MiGA challenge and workshop on micro-gesture analysis for hidden emotion understanding (MiGA) to fill the gap in the current research field. The MiGA workshop and challenge aim to promote research on developing AI methods for MG analysis toward the goal of hidden emotion understanding. As the proposed MiGA will be the second event, we propose to hold it in workshop + challenge mode with a focus on the competition part that builds and provides benchmark datasets and a fair validation platform for researchers working in the MG classification and online recognition for identity-insensitive emotion understanding. The workshop covers a wider scope than the challenge, i.e., any research that provides theoretical and practical support for gesture and micro-gesture analysis and emotion understanding.</p>
        </p>

        <div class="profess-cover row no-margin">
            <div class="col-md-6">

            </div>
        </div>
                                         <div>

                                          <h5> Workshop Topics </h5>
                                          <ul>
                                            The workshop supplementing the challenge covers a wider scope, i.e., any paper that is related to gesture and micro-gesture analysis for emotion understanding but is not directly about any of the challenge tracks can be submitted as a workshop paper. The topic includes but is not limited to:
                                            <div style="margin-left: 20px; text-align: justify;">
                                            <li style="list-style: disc;">Gesture and micro-gesture analysis for emotion understanding.</li>
                                              <li style="list-style: disc;">Vision-based methodologies for gesture-based emotion understanding, e.g., classification, detection, online recognition, generation, and transferring.</li>
                                              <li style="list-style: disc;">Solutions for special challenges involved with the in-the-wild gesture analysis, e.g., severely imbalanced sample distribution, high heterogeneous samples of interclass, noisy irrelevant motions, noisy backgrounds, etc.</li>
                                              <li style="list-style: disc;">Different modalities developed for emotion understanding, e.g., body gestures, attentive gazes, and desensitized voices.</li>
                                              <li style="list-style: disc;">New data collected for the purpose of hidden emotion understanding.</li>
                                              <li style="list-style: disc;">Psychological study and neuroscience research about various body behaviors and their links to emotions.</li>
                                              <li style="list-style: disc;">Hardware/apparatuses/imaging systems developed for the purpose of hidden emotion understanding.</li>
                                              <li style="list-style: disc;">Applications of gestures and micro-gestures, e.g., for medical assessment in hospitals (ADHD, depression), for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc.</li>
                                            </div>
                                            </ul>

                                   <div class="profess-cover row no-margin">
                                       <div class="col-md-6">

                                       </div>
                                   </div>
                                     <h5> Submission Guidelines </h5>

                                     <p>- Papers must comply with the <b><a style="color:blue;" href="https://ceur-ws.org/HOWTOSUBMIT.html/"> CEURART paper style</a> </b> (1 column) and can fall in one of the following categories:</p>
                                     <p>- Full research papers(minimum 7 pages)</p>
                                     <p>- Short research papers(4-6 pages)</p>
                                     <p>- Position papers(2 pages)</p>

                                     <p>- The CEURART template can be found on this <b><a style="color:blue;" href="https://it.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw/"> Overleaf link</a> </b>.</p>
                                     <p>- Accepted papers (after blind review of at least 3 experts) will be included in a volume of the CEUR Workshop Proceedings. We are also planning to organize a special issue and the authors of the most interesting and relevant papers will be invited to submit and extended manuscript.</p>

                                    <p>- Workshop submissions will be handled by CMT submission system; the submission link is as follows: <b><a style="color:blue;" href="https://cmt3.research.microsoft.com/MiGAIJCAI2024"> Paper Submission</a> </b>. All questions about submissions should be emailed to chen.haoyu at oulu.fi</p>

                                    <div class="profess-cover row no-margin">
                                        <div class="col-md-6">

                                        </div>
                                    </div>
                                    <h5> Workshop Important Dates (might slightly adjust later) </h5>
                                   <ul>
                                     <li>June 02, 2024. Paper submission deadline.</li>
                                     <li> June 06, 2024. Notification to authors.</li>
                                     <li> June 10, 2024. Camera-ready deadline.</li>
                                     <li> August, 2024. MiGA IJCAI 2024 Workshop, Jeju, South Korea.</li>
                                   </ul>

                                   <p>Note: Each paper must be presented on-site by an Author/co-Author at the conference.</p>


                                   <div class="profess-cover row no-margin">
                                       <div class="col-md-6">

                                       </div>
                                   </div>
                                   <h5> Workshop Program </h5>
                                   <p>The proposed workshop (including the challenge) will be held as a full-day event, August --, 2024.</p>
                                <ul>

                                  </ul>
                                    <!-- <img src="assets/images/programme.PNG" alt=""> -->
                                 </div>

                    </div>
              </div>
              <div class="tab-pane fade exp-cover" id="resume" role="tabpanel" aria-labelledby="profile-tab">
                <div class="data-box" style="text-align: justify;">
                  <div class="sec-title">
                                        <h2 class="decorated"><span>Challenge Details</span></h2>
                                  </div>

                                   <!-- <div class="service no-margin row">
                                          <div class="col-sm-3 resume-dat serv-logo">
                                              <h6>2013-2015</h6>
                                            <p>Master Degree</p>
                                          </div>
                                          <div class="col-sm-9 rgbf">
                                              <h5>Cambridg University</h5>
                                              <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                          </div>
                                      </div>
                                      <div class="service no-margin row">
                                          <div class="col-sm-3 resume-dat serv-logo">
                                              <h6>2013-2015</h6>
                                            <p>Bacholers Degree</p>
                                          </div>
                                          <div class="col-sm-9 rgbf">
                                              <h5>Anna University</h5>
                                              <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                          </div>
                                      </div>
                                      <div class="service no-margin row">
                                          <div class="col-sm-3 resume-dat serv-logo">
                                              <h6>2013-2015</h6>
                                            <p>High School</p>
                                          </div>
                                          <div class="col-sm-9 rgbf">
                                              <h5>A.M.H.S.S</h5>
                                              <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                          </div>
                                      </div>
                                      <div class="service no-margin row">
                                          <div class="col-sm-3 resume-dat serv-logo">
                                              <h6>2013-2015</h6>
                                            <p>School</p>
                                          </div>
                                          <div class="col-sm-9 rgbf">
                                              <h5>Anna University</h5>
                                              <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                          </div>
                                      </div> -->

                                      <p> The MiGA challenge is planned as a continuous, annual event, and each years MiGA will include different (multiple) challenge tasks, with large-scale datasets with expanding size. This year in MiGA, we will focus on two fundamental tasks of micro-gestures: <b>classification and online recognition</b>. The tracks of MiGA will be extended to leveraging those identity-insensitive cues to achieve hidden emotion understanding in the future. The challenge will be based on two spontaneous datasets: One is the SMG dataset published in FG2019 “Analyze Spontaneous Gestures for Emotional Stress State Recognition: A Microgesture Dataset and Analysis with Deep Learning” and the other is the iMiGUE dataset published in CVPR2021 “iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis”. In this MiGA, we plan to set two fundamental challenge tasks (Tracks), and participating teams can choose to compete on one or both tasks. The challenge will be organized on the CodaLab website. </p>


                                       <p><b>Track 1: </b>Micro-gesture classification from short video clips. The MG datasets were collected from in-the-wild settings. Compared to ordinary action/gesture data, MGs concern more fine-grained and subtle body movements that occur spontaneously in practical interactions. Thus, learning those fine-grained body movement patterns, handling imbalanced sample distribution of MGs, and distinguishing the high heterogeneous MG samples of interclass are the big challenges to be addressed.</p>

                                       <p><b>Track 2: </b>Online micro-gesture recognition from long video sequences. Unlike any existing online action/gesture recognition datasets in which samples are well aligned/performed in the sequence, MGs samples occur spontaneously in any combinations or orders just like seen in daily communicative scenarios. Thus, the task of online micro-gesture recognition requires dealing with more complex body-movement transition patterns (e.g., co-occurrence of multiple MGs, incomplete MGs and complicated transitions between MGs, etc.) and detecting fine-grained MGs from irrelevant/context body movements, which poses new challenges that havent been considered in previous gesture research.</p>
                                           <p></p>
                                           <div class="profess-cover row no-margin">
                                               <div class="col-md-6">

                                               </div>
                                           </div>
                                           <h5>The rules and guidelines for the competition/challenge</h5>

                                           <b>1. Datasets</b><br>
                                           <p> Datasets for the proposed challenge are available. The MiGA challenge is planned as a continuous annual event. Two benchmark datasets published on FG 2019 and CVPR 2021 are available and will be used for the challenge. The first one is the SMG dataset published in FG2019. Spontaneous Micro-Gesture (SMG) dataset consists of 3,692 samples of 17 MGs. The MG clips are annotated from 40 long video sequences (10-15 minutes) with 821,056 frames in total. The datasets were collected from 40 subjects while narrating a fake and real story to elicit the emotional states. The participants are recorded collected by Kinect resulting in four modalities, RGB, 3D skeletal joints, depth and silhouette.  In this workshop, we allow participants to use the skeleton, RGB or both modalities. <a style="color: blue;" href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis"> Download SMG dataset</a> </p>
                                               <br>
                                                 <p> The second dataset is iMiGUE published in CVPR2021. Micro-Gesture Understanding and Emotion analysis (iMiGUE) dataset consists of 32 MGs plus one non-MG class collected from post-match press conferences videos of famous tennis players. The dataset consists of 18,499 samples of MGs to detect negative and positive emotions. The MG clips are annotated from 359 long video sequences (0.5-26 minutes) with 3,765,600 frames in total. The dataset contains RGB modality and 2D skeletal joints collected from Open-Pose. In this workshop, we allow participants to use the
                                                  skeleton, RGB or both modalities.
                                                 <a style="color: blue;" href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis"> Download iMiGUE dataset</a></p>
                                                 <br>
                                               <p>Note that: 1) not all data are used for the challenge. 2) part of the data will be selected and tailored for different challenge tasks, 3) planned as a continuous event, MiGA will contain multiple parallel challenge tracks and the tasks vary in each year.</p> <br>
                                           <b>2. Evaluation</b><br>
                                           <p> Training datasets include iMiGUE and SMG datasets while the testing dataset (without annotation released) is from iMiGUE. We deploy a cross-subject evaluation protocol by dividing the 72 subjects into a training group of 37 subjects and a testing group of 35 subjects. For MG classification track, 13,936 and 3,692 MG clips from iMiGUE and SMG datasets will be used for training and validating, and the remaining 4,563 MG clips from iMiGUE will be used for testing. For MG online recognition track, 252 and 40 long sequences from iMiGUE and SMG datasets will be used for training and validating, and the remaining 104 long sequences from iMiGUE will be used for testing. </p>
                                            <br>
                                            <p><b>MG classification track:</b> We report Top-5 accuracy on the following subsets of the test set: 1) Overall: All segments in the test split; 2) Tail Classes: Due to the long-tailed nature of the datasets, among a total 33 classes in iMiGUE dataset, 28 classes are tail classes (approx. 57.8of the data). Since there is considerable overlap between consecutive fine-grained MG segments, we use Top-5 accuracy instead of the standard Top-1. Submissions will be ranked on the basis of Top-5 accuracy on the overall split.</p>
                                            <br>
                                            <p><b>MG online recognition track:</b> We jointly evaluate the detection and classification performances of algorithms by using the F1 score measurement defined below: F1 =2*Precision*Recall/(Precision+Recall), given a long video sequence that needs to be evaluated, P recision is the fraction of correctly classified MGs among all gestures retrieved in the sequence by algorithms, while Recall (or sensitivity) is the fraction of MGs that have been correctly retrieved over the total
                                            amount of annotated MGs.</p>
                                           <br>
                                           <p>Submission format for both tracks. Participants must submit their predictions in the following format. For each sequence SequenceXXXX.zip in the data folder, participants should create a SequenceXXXX prediction.csv file with a line for each predicted gesture [GestureID, StartFrame, EndFrame] (the same format as SequenceXXXX labels.csv) (for classification track, the StartFrame and EndFrame are empty). The predictions for all the samples will be put in a single ZIP file and submitted to Codalab. The results will be evaluated on the server and displayed on the ranking list in real-time. The organization team has the right to examine the participants source code to ensure the reproducibility of the algorithms. The final results and ranking will be confirmed
                                            and announced by organizers. </p>
                                           <br>

                                           <div class="profess-cover row no-margin">
                                               <div class="col-md-6">

                                               </div>
                                           </div>
                                           <h5>Participation guidelines</h5>
                                           Please visit the Codalab websites to join the competitions:

                                           <p>  <a style="color: blue;" href="https://codalab.lisn.upsaclay.fr/competitions/11758"> The 2nd MiGA-IJCAI Challenge Track 1: Micro-gesture Classification</a> </p>

                                           <p>  <a style="color: blue;" href="https://codalab.lisn.upsaclay.fr/competitions/11782"> The 2nd MiGA-IJCAI Challenge Track 2: Micro-gesture Online Recognition</a> </p>
                                           <div class="profess-cover row no-margin">
                                               <div class="col-md-6">

                                               </div>
                                           </div>

                                            <h5>Final results will be available soon</h5>
                                            <p style="padding-top: 50px"> </p>
                                           <!-- We have examined all source code submitted and finalized the rankings of the MiGA challenge. Congratulations to the following competitors on their final rankings:
                                           <p>Track 1: Micro-gesture Classification </p>
                                           <p>1. gkdx2, team name 'HFUT-VUT' </p>
                                           <p>2. HHuang, team name 'NPU-Stanford' </p>
                                           <p>3. ChenxiCui </p>

                                           <p>Track 2: Micro-gesture Online Recognition</p>
                                           <p>1. HHuang, team name 'NPU-Stanford' </p>
                                           <p>2. gkdx2, team name 'HFUT-VUT' </p>
                                           <div class="profess-cover row no-margin">
                                               <div class="col-md-6">

                                               </div>
                                           </div> -->

                    <h5>Important Dates (might slightly adjust later)</h5>
                    <ul>
                        The timeline for the Challenge will be organized as follows:
                        <li>Mar 27, 2024. Call for Challenge online. Registration starts.</li>
                        <li>Mar 31, 2024. Release of training data.</li>
                        <li>May 2, 2024. Release of testing data. </li>
                        <li>May 12, 2024. Final testing data and result submission. Registration ends.</li>
                        <li>May 15, 2024. Release of challenge results.</li>
                        <li>June 02, 2024. Paper submission deadline (workshop).</li>
                        <li> June 06, 2024. Notification to authors.</li>
                        <li> June 10, 2024. Camera-ready deadline. </li>
                        <li> August, 2024. MiGA IJCAI 2024 Workshop, Jeju, Korea. </li>
                    </ul>
</div>
              </div>
              <div class="tab-pane fade gallcoo" id="gallery" role="tabpanel" aria-labelledby="contact-tab">

                <div class="sec-title">
                  <h2 class="decorated"><span>Organizing Committee</span></h2>
                </div>

                <div class="cards">
                  <div class="row justify-content-center">

                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://gyzhao-nm.github.io/Guoying/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/guoying-1.JPG" />
                            <h4>Guoying Zhao</h4>
                            <!-- <h5>University of Oulu, FI</h5>-->
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a>

                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="http://www.schuller.one/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/bjorn.png" />
                            <h4>Björn W. Schuller</h4>
                            <p>University of Augsburg, DE <br>  Imperial College London, UK</p>
                        </div>
                      </div>
                    </div>
                  </a>

                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://stanford.edu/~eadeli/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/ehsan.JPG" />
                            <h4>Ehsan Adeli</h4>
                            <p>Stanford University, USA <br>  </p>
                        </div>
                      </div>
                    </div>
                  </a>

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://github.com/mikecheninoulu/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/chenhaoyu1.png" />
                            <h4>Haoyu Chen</h4>
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a>

                  <!-- <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="http://sourcedb.psych.cas.cn/en/epsychexpert/201206/t20120628_3606154.html">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/tingshao-1.JPG" />
                            <h4>Tingshao Zhu</h4>
                            <p>Institute of Psychology, Chinese Academy of Sciences <br>  </p>
                        </div>
                      </div>
                    </div>
                  </a>-->

                </div>
                </div>

                <div class="sec-title">
                  <h2 class="decorated"><span>Invited speakers</span></h2>
                  TBD
                </div>

                <div class="cards">
                  <div class="row justify-content-center">

                  <!-- Person -->

                  <!-- <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://profiles.stanford.edu/f-lin/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/vankee.jpg" />
                            <h4>Feng Vankee Lin</h4>
                            <p>Stanford University, USA <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a> -->

                </div>
                </div>

                <div class="sec-title">
                  <h2 class="decorated"><span>Data Chairs</span></h2>
                </div>

                <div class="cards">
                  <div class="row justify-content-center">

                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://github.com/mikecheninoulu/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src=" " />
                            <h4>Atif Shah</h4>
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a>
                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://github.com/marukosan93/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/marko.jpg" />
                            <h4>Marko Savic</h4>
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a>
                  <!-- Person -->

                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://github.com/leoamb">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="" />
                            <h4>Abdelrahman Mostafa</h4>
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a>
                  <!-- Person -->


                  <!-- <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://sites.google.com/view/xinliu/">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/xinliu.png" />
                            <h4>Xin Liu</h4>
                            <p>Lappeenranta-Lahti University of Technology LUT, FI   </p>
                        </div>
                      </div>
                    </div>
                  </a>


                  <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://www.oulu.fi/en/researchers/xiaobai-li">
                    <div class="card-body">
                      <div class="card-front">
                        <div class="text-center">
                            <img class="img-fluid"  src="assets/organizers/xiaobaili.png" />
                            <h4>Xiaobai Li</h4>
                            <p>University of Oulu, FI <br>   </p>
                        </div>
                      </div>
                    </div>
                  </a> -->
                </div>
                </div>


                                      <!-- <div class="sec-title">
                                                            <h2 class="decorated"><span>Challenge Technical Support</h2>
                                                      </div>
                                        <div class="row no-margin gallery">
                                                                <div class="col-sm-4">
                                                                    <img src="assets/organizers/guoying-1.JPG" alt="">
                                                                    <p><a style="color:blue;" href="https://scholar.google.com/citations?user=QgbraMIAAAAJ&hl=en"> Guoying Zhao</a>, University of Oulu, FI.</p>
                                                                </div>
                                                                <div class="col-sm-4">
                                                                    <img src="assets/organizers/bjorn-1.JPG" alt="">
                                                                    <p><a style="color:blue;" href="http://www.schuller.one/">Björn W. Schuller</a>, University of Augsburg, Germa-ny; Imperial College London, London/UK.</p>
                                                                </div>
                                                                <div class="col-sm-4">
                                                                    <img src="assets/organizers/ehsan.JPG" alt="">
                                                                    <p><a style="color:blue;" href="https://stanford.edu/~eadeli/">Ehsan Adeli</a>, Stanford University, USA.</p>
                                                                </div>
                                                                <div class="col-sm-4">
                                                                    <img src="assets/organizers/tingshao-1.JPG" alt="">
                                                                    <p><a style="color:blue;" href="">Tingshao Zhu</a>, Institute of Psychology, Chinese Academy of Sciences.</p>
                                                                </div>


                                                            </div> -->
              </div>
              <div class="tab-pane fade contact-tab" id="contact" role="tabpanel" aria-labelledby="contact-tab">
                <div class="sec-title">
                  <h2 class="decorated"><span>Contact us</span></h2>
                </div>


                  <div class="row no-margin">
                                          <div class="col-md-6 no-padding" style="text-align: center;">
                                            <img src="assets/qr/qr_discord_2024.jpeg" alt="" width="300" height="300">
                                            <p>Welcome to our Discord Channel and discuss with peers. <br> <a style="color:blue;" href="https://discord.gg/XPrvM8WjnP">https://discord.gg/XPrvM8WjnP</a> </p>

                                          </div>

                                          <div class="col-md-6">
                                            <ul>
                                                The contact info is listed as follows:
                                                <li>• For questions regarding <b>workshop submissions and the competition</b>, please get in touch with chen.haoyu at oulu.fi.</li>
                                                <li>• For questions about the <b>workshop local arrangements and information</b>, please get in touch with local@ijcai-23.org.</li>
                                                <li>• For questions regarding <b>general issue of the workshp program</b>, please get in touch with guoying.zhao at oulu.fi. </li>
                                            </ul>
                                          </div>
                                      </div>
              </div>
            </div>
        </div>
    </div>
</body>

<script src="assets/js/jquery-3.2.1.min.js"></script>
<script src="assets/js/popper.min.js"></script>
<script src="assets/js/bootstrap.min.js"></script>
<script src="assets/js/script.js"></script>


</html>
